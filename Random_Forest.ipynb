{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load libraries\n",
    "import numpy as np\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load saved training/validation/test data\n",
    "with open('train_val_test_data.pkl', 'rb') as file:\n",
    "    X_train, X_val, X_test, y_train, y_val, y_test = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select specific features of the input passwords\n",
    "rf_features = ['length', 'uppercase_count', 'lowercase_count', 'numbers_count', 'special_character_count', 'entropy']\n",
    "X_train = X_train[rf_features].to_numpy()\n",
    "X_val = X_val[rf_features].to_numpy()\n",
    "X_test = X_test[rf_features].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# flatten the label data\n",
    "y_train = y_train.flatten()\n",
    "y_val = y_val.flatten()\n",
    "y_test = y_test.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate accuracy given the true labels and the predictions\n",
    "def accuracy(y_truth, y_pred):\n",
    "    correct_pred = 0\n",
    "    # iterate through the values and check if the labels are the same, update as required\n",
    "    for y_t, y_p in zip(y_truth, y_pred):\n",
    "        if y_t == y_p :\n",
    "            correct_pred += 1\n",
    "    # find the proportion by dividing the correct predictions by all the predictions\n",
    "    return correct_pred / len(y_truth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate Gini impurity, calculate error based on random classification \n",
    "def gini_impurity(y):\n",
    "    # find the counts of each class/label\n",
    "    class_counts = np.bincount(y.flatten())\n",
    "    # find the distribution of samples across the classes\n",
    "    probabilities = class_counts / len(y)\n",
    "    # calculate the error and return\n",
    "    return 1 - np.sum(probabilities ** 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate Shannon Entropy, measures unpredictability of password\n",
    "def entropy(y):\n",
    "    # find the counts of each class/label\n",
    "    class_counts = np.bincount(y.flatten())\n",
    "    # find the distribution of samples across the classes\n",
    "    probabilities = class_counts / len(y)\n",
    "    # to avoid log(0), add a small value to zero probabilities\n",
    "    return -np.sum(probabilities * np.log2(probabilities + 1e-10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# finds the thresholds given the column of values and number of thresholds and indication of randomness\n",
    "def calculate_thresholds(feature_column, n_thresholds, extra_trees):\n",
    "    # if using extra trees, pick random thresholds from continuous range\n",
    "    if extra_trees:\n",
    "        # find minimum and maximum of this feature's values\n",
    "        feature_min = feature_column.min()\n",
    "        feature_max = feature_column.max()\n",
    "        # if there is only one possible value, just use that as the threshold\n",
    "        if feature_min == feature_max:\n",
    "            return np.unique(feature_column)\n",
    "        # otherwise pick random thresholds from a continuous range between minimum and maximum\n",
    "        else:\n",
    "            return np.random.uniform(feature_min, feature_max, size=n_thresholds or 10)\n",
    "    # use random sampling of thresholds if n_thresholds is specified\n",
    "    elif n_thresholds is not None:\n",
    "        # randomly sample n thresholds from unique values of feature column\n",
    "        return np.random.choice(np.unique(feature_column), min(n_thresholds, len(np.unique(feature_column))), replace=False)\n",
    "    else:\n",
    "        # find all unique values in the feature column\n",
    "        return np.unique(feature_column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate a single split to see what the impurity for this feature's threshold will be\n",
    "def evaluate_split(feature_column, y, threshold, criterion):\n",
    "    # find all the samples whose values in the feature column are less than the threshold\n",
    "    left_node_samples = y[feature_column < threshold]\n",
    "    # find all the samples whose values in the feature column are greater than or equal to the threshold\n",
    "    right_node_samples = y[feature_column >= threshold]\n",
    "    \n",
    "    # if there are samples on either side of the threshold value, then calculate the impurity and update as required\n",
    "    # essentially filters out invalid thresholds that may be greater/less than all the samples\n",
    "    if len(left_node_samples) != 0 and len(right_node_samples) != 0:\n",
    "        # use the appropriate criterion\n",
    "        if criterion == 'entropy':\n",
    "                # calculate the entropy of the left node samples\n",
    "            left_impurity = entropy(left_node_samples)\n",
    "            # calculate the entropy of the right node samples\n",
    "            right_impurity = entropy(right_node_samples)\n",
    "        elif criterion == 'gini':\n",
    "            # calculate the impurity of the left node samples\n",
    "            left_impurity = gini_impurity(left_node_samples)\n",
    "            # calculate the impurity of the right node samples\n",
    "            right_impurity = gini_impurity(right_node_samples)\n",
    "        # find the weighted impurity value (expectation)\n",
    "        return (len(left_node_samples) * left_impurity + len(right_node_samples) * right_impurity) / len(y)\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate the splits to see which features and thresholds will be the best\n",
    "def evaluate_splits(feature_column, y, thresholds, criterion, min_impurity, feature):\n",
    "    # for each threshold value, it is trying to minimize the gini impurity\n",
    "    for threshold in thresholds:\n",
    "        # find the impurity of the this threshold for this feature\n",
    "        weighted_impurity = evaluate_split(feature_column, y, threshold, criterion)\n",
    "        # if the weighted impurity is less than the current minimum impurity, then update the values\n",
    "        if weighted_impurity is not None and weighted_impurity < min_impurity:\n",
    "            min_impurity = weighted_impurity\n",
    "            min_impurity_feature = feature\n",
    "            min_impurity_threshold = threshold\n",
    "    return min_impurity_feature, min_impurity_threshold, min_impurity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# finds the best split in the tree\n",
    "def best_split(X, y, n_features, n_thresholds=None, criterion='gini', multivariate_split=False, extra_trees=False):\n",
    "    # find the number of features for each password\n",
    "    num_features = X.shape[1]\n",
    "    # initialize the minimum impurity (begin with infinity to find smallest impurity)\n",
    "    min_impurity = float('inf')\n",
    "    # initialize the specific feature associated with the minimum impurity\n",
    "    min_impurity_feature = None\n",
    "    # initialize the minimum impurity feature's corresponding threshold value\n",
    "    min_impurity_threshold = None\n",
    "    # initialize weights to be None\n",
    "    weights = None\n",
    "\n",
    "    # choose a random subset of features from all the features\n",
    "    feature_subset = np.random.choice(num_features, n_features, replace=False)\n",
    "\n",
    "    # if multivariate, split based on linear combination of features subset (helps if data is not separated by single feature)\n",
    "    if multivariate_split:\n",
    "        # create random weights for each feature in subset\n",
    "        weights = np.random.uniform(-1, 1, size=n_features)\n",
    "        # combine features in subset with weights\n",
    "        linear_combination = X[:, feature_subset].dot(weights)\n",
    "        # find possible threshold values for splitting using the linear combinations\n",
    "        thresholds = calculate_thresholds(linear_combination, n_thresholds, extra_trees)\n",
    "\n",
    "        # evaluate the split\n",
    "        min_impurity_feature, min_impurity_threshold, min_impurity = evaluate_splits(linear_combination, y, thresholds, criterion, min_impurity, feature_subset)\n",
    "    # otherwise, do univariate split for each feature in subset\n",
    "    else:\n",
    "        # iterate through the features in the subset\n",
    "        for feature in feature_subset:\n",
    "            # get the column corresponding to the current feature\n",
    "            feature_column = X[:, feature]\n",
    "            # find possible threshold values for splitting\n",
    "            thresholds = calculate_thresholds(feature_column, n_thresholds, extra_trees)\n",
    "\n",
    "            # evaluate the split\n",
    "            min_impurity_feature, min_impurity_threshold, min_impurity = evaluate_splits(feature_column, y, thresholds, criterion, min_impurity, feature_subset)\n",
    "\n",
    "    return min_impurity_feature, min_impurity_threshold, weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# builds the decision tree\n",
    "def build_tree(X, y, num_classes, max_depth, min_samples_split, n_features, depth=0,  n_thresholds=None, criterion='gini', soft_voting=False, multivariate_split=False, extra_trees=False):\n",
    "    # if there are no unique y values or there are fewer samples than required to split the tree \n",
    "    # or the depth is greater than the specified max depth, return the majority class of the samples (regular voting)\n",
    "    if len(np.unique(y)) == 1 or len(y) < min_samples_split or depth >= max_depth:\n",
    "        class_counts = np.bincount(y.flatten(), minlength=num_classes)\n",
    "        # if using soft voting, store class distribution at leaf node\n",
    "        if soft_voting:\n",
    "            return class_counts / len(y)\n",
    "        # otherwise return the majority class for regular voting\n",
    "        else:\n",
    "            return np.argmax(np.bincount(y.flatten()))\n",
    "\n",
    "    # find the feature and threshold that corresponds to the best split\n",
    "    feature, threshold, weights = best_split(X, y, n_features, n_thresholds=n_thresholds, criterion=criterion, multivariate_split=multivariate_split, extra_trees=extra_trees)\n",
    "\n",
    "    # if no feature is returned, return the majority class of the samples\n",
    "    if feature is None:\n",
    "        return np.argmax(np.bincount(y.flatten(), minlength=num_classes))\n",
    "\n",
    "    # get the column corresponding to the feature\n",
    "    feature_column = X[:, feature]\n",
    "\n",
    "    # find all the samples whose values in the feature column are less than the threshold\n",
    "    left_node_samples_X = X[feature_column < threshold]\n",
    "    left_node_samples_y = y[feature_column < threshold]\n",
    "    \n",
    "    # find all the samples whose values in the feature column are greater than or equal to the threshold\n",
    "    right_node_samples_X = X[feature_column >= threshold]\n",
    "    right_node_samples_y = y[feature_column >= threshold]\n",
    "\n",
    "    # recursively build the left and right sides of the tree\n",
    "    left_tree = build_tree(left_node_samples_X, left_node_samples_y, num_classes, max_depth, min_samples_split, n_features=n_features, depth=depth + 1, n_thresholds=n_thresholds, criterion=criterion, soft_voting=soft_voting, multivariate_split=multivariate_split, extra_trees=extra_trees)\n",
    "    right_tree = build_tree(right_node_samples_X, right_node_samples_y, num_classes, max_depth, min_samples_split, n_features=n_features, depth=depth + 1, n_thresholds=n_thresholds, criterion=criterion, soft_voting=soft_voting, multivariate_split=multivariate_split, extra_trees=extra_trees)\n",
    "\n",
    "    return (feature, threshold, left_tree, right_tree, weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# classify sample with the decision tree\n",
    "def predict_tree(x, tree):\n",
    "    # if the given tree is not a tuple, that means either the class probabilities or the majority class was returned so return the prediction\n",
    "    if not isinstance(tree, tuple):\n",
    "        return tree\n",
    "    \n",
    "    # unpack the tuple\n",
    "    feature, threshold, left_tree, right_tree, weights = tree\n",
    "\n",
    "    # if there are weights, then it is multivariate so find the linear combination otherwise just find the feature value\n",
    "    if weights is not None:\n",
    "        compare_threshold = x[feature].dot(weights)\n",
    "    else:\n",
    "        compare_threshold = x[feature]\n",
    "\n",
    "    # determine which side of the tree to iterate based on relation of previously found threshold\n",
    "    if compare_threshold < threshold:\n",
    "        return predict_tree(x, left_tree)\n",
    "    else:\n",
    "        return predict_tree(x, right_tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trains random forest\n",
    "def random_forest(X, y, num_classes, n_trees, max_depth, min_samples_split, max_features, n_thresholds=None, criterion='gini', soft_voting=False, multivariate_split=False, extra_trees=False):\n",
    "    # initialize list of trees\n",
    "    trees = []\n",
    "    # create n number of decision trees\n",
    "    for tree in range(n_trees):\n",
    "        # sample with replacement to increase diversity and reduce overfitting\n",
    "        sample_indices = np.random.choice(len(X), len(X), replace=True)\n",
    "        # build a decision tree with the randomly chosen samples\n",
    "        tree = build_tree(X[sample_indices], y[sample_indices], num_classes, max_depth, min_samples_split, n_features=max_features, n_thresholds=n_thresholds, criterion=criterion, soft_voting=soft_voting, multivariate_split=multivariate_split, extra_trees=extra_trees)\n",
    "        # store the decision tree\n",
    "        trees.append(tree)\n",
    "    return trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# classify samples with the random forest\n",
    "def predict_forest(X, trees, soft_voting=False):\n",
    "    predictions = []\n",
    "    # get the prediction from each tree for each sample\n",
    "    for x in X:\n",
    "        tree_predictions = np.array([predict_tree(x, tree) for tree in trees])\n",
    "\n",
    "        # if soft voting, then average class probabilities and return class with highest probability\n",
    "        if soft_voting:\n",
    "            average_class_probs = np.mean(tree_predictions, axis=0)\n",
    "            predictions.append(np.argmax(average_class_probs))\n",
    "        else:\n",
    "            # get the majority class for each sample as a list\n",
    "            predictions.append(np.argmax(np.bincount(tree_predictions)))\n",
    "\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "trees = random_forest(X_train, y_train, 5, 100, 45, 5, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Train Accuracy: 0.9983875\n",
      "Random Forest Validation Accuracy: 0.9987\n",
      "Random Forest Test Accuracy: 0.9978\n"
     ]
    }
   ],
   "source": [
    "# find the accuracy metrics for each set of data using Random Forest\n",
    "train_predictions = predict_forest(X_train, trees)\n",
    "train_accuracy = accuracy(y_train, train_predictions)\n",
    "print(\"Random Forest Train Accuracy:\", train_accuracy)\n",
    "\n",
    "val_predictions = predict_forest(X_val, trees)\n",
    "val_accuracy = accuracy(y_val, val_predictions)\n",
    "print(\"Random Forest Validation Accuracy:\", val_accuracy)\n",
    "\n",
    "test_predictions = predict_forest(X_test, trees)\n",
    "test_accuracy = accuracy(y_test, test_predictions)\n",
    "print(\"Random Forest Test Accuracy:\", test_accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
