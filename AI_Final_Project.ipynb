{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from nltk.util import ngrams\n",
    "import math\n",
    "import re\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import heapq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>password</th>\n",
       "      <th>strength</th>\n",
       "      <th>length</th>\n",
       "      <th>class_strength</th>\n",
       "      <th>entropy</th>\n",
       "      <th>crack_time_sec</th>\n",
       "      <th>crack_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>bybee</td>\n",
       "      <td>0.088053</td>\n",
       "      <td>5</td>\n",
       "      <td>Very week</td>\n",
       "      <td>11.60964</td>\n",
       "      <td>1.562500e-06</td>\n",
       "      <td>instant</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>n3m0</td>\n",
       "      <td>0.088889</td>\n",
       "      <td>4</td>\n",
       "      <td>Very week</td>\n",
       "      <td>8.00000</td>\n",
       "      <td>1.280000e-07</td>\n",
       "      <td>instant</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2509</td>\n",
       "      <td>0.088889</td>\n",
       "      <td>4</td>\n",
       "      <td>Very week</td>\n",
       "      <td>8.00000</td>\n",
       "      <td>1.280000e-07</td>\n",
       "      <td>instant</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4622</td>\n",
       "      <td>0.070443</td>\n",
       "      <td>4</td>\n",
       "      <td>Very week</td>\n",
       "      <td>8.00000</td>\n",
       "      <td>1.280000e-07</td>\n",
       "      <td>instant</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>shrk</td>\n",
       "      <td>0.088889</td>\n",
       "      <td>4</td>\n",
       "      <td>Very week</td>\n",
       "      <td>8.00000</td>\n",
       "      <td>1.280000e-07</td>\n",
       "      <td>instant</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  password  strength  length class_strength   entropy  crack_time_sec  \\\n",
       "0    bybee  0.088053       5      Very week  11.60964    1.562500e-06   \n",
       "1     n3m0  0.088889       4      Very week   8.00000    1.280000e-07   \n",
       "2     2509  0.088889       4      Very week   8.00000    1.280000e-07   \n",
       "3     4622  0.070443       4      Very week   8.00000    1.280000e-07   \n",
       "4     shrk  0.088889       4      Very week   8.00000    1.280000e-07   \n",
       "\n",
       "  crack_time  \n",
       "0    instant  \n",
       "1    instant  \n",
       "2    instant  \n",
       "3    instant  \n",
       "4    instant  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load password dataset\n",
    "df_passwords = pd.read_csv('dataset.csv')\n",
    "df_passwords.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0        0\n",
      "1        0\n",
      "2        0\n",
      "3        0\n",
      "4        0\n",
      "        ..\n",
      "99995    4\n",
      "99996    4\n",
      "99997    4\n",
      "99998    4\n",
      "99999    4\n",
      "Name: numeric_class_strength, Length: 100000, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "dictEncoding = {'very week': 0, 'week': 1, 'average': 2, 'strong': 3, 'very strong': 4}\n",
    "df_passwords['numeric_class_strength'] = df_passwords['class_strength'].str.lower().map(dictEncoding)\n",
    "labels = df_passwords['numeric_class_strength']\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate ngrams from 1 to 5\n",
    "def generate_ngrams_counts(character_tokens):\n",
    "    all_ngrams = []\n",
    "    for i in range(5):\n",
    "        all_ngrams.extend([''.join(gram) for gram in ngrams(character_tokens, i + 1)])\n",
    "    return Counter(all_ngrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate shannon_entropy given character counts\n",
    "def shannon_entropy(character_tokens):\n",
    "    character_counts = Counter(character_tokens)\n",
    "    total = len(character_tokens)\n",
    "    return -sum((count / total) * math.log2(count / total) for count in character_counts.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find counts of special characters in the password\n",
    "def count_special_characters(password):\n",
    "    # matches any character that is NOT a letter, number, or whitespace\n",
    "    pattern = r\"[^a-zA-Z0-9\\s]\"  \n",
    "    return len(re.findall(pattern, password))\n",
    "\n",
    "# find counts of numbers in the password\n",
    "def count_numbers(password):\n",
    "    # matches any digit (0-9)\n",
    "    pattern = r\"\\d\" \n",
    "    return len(re.findall(pattern, password))\n",
    "\n",
    "# finds counts of uppercase letters in the password\n",
    "def count_uppercase(password):\n",
    "    # matches any uppercase letter (A-Z)\n",
    "    pattern = r\"[A-Z]\"  \n",
    "    return len(re.findall(pattern, password))\n",
    "\n",
    "# finds counts of lowercase letters in the password\n",
    "def count_lowercase(password):\n",
    "    # matches any lowercase letter (a-z)\n",
    "    pattern = r\"[a-z]\"  \n",
    "    return len(re.findall(pattern, password))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate tf-idf for the character counts in the password\n",
    "# tbd ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>password</th>\n",
       "      <th>length</th>\n",
       "      <th>character_tokens</th>\n",
       "      <th>uppercase_count</th>\n",
       "      <th>lowercase_count</th>\n",
       "      <th>numbers_count</th>\n",
       "      <th>special_character_count</th>\n",
       "      <th>ngram_occurrences</th>\n",
       "      <th>entropy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>bybee</td>\n",
       "      <td>5</td>\n",
       "      <td>[b, y, b, e, e]</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>{'b': 2, 'y': 1, 'e': 2, 'by': 1, 'yb': 1, 'be...</td>\n",
       "      <td>1.521928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>n3m0</td>\n",
       "      <td>4</td>\n",
       "      <td>[n, 3, m, 0]</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>{'n': 1, '3': 1, 'm': 1, '0': 1, 'n3': 1, '3m'...</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2509</td>\n",
       "      <td>4</td>\n",
       "      <td>[2, 5, 0, 9]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>{'2': 1, '5': 1, '0': 1, '9': 1, '25': 1, '50'...</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4622</td>\n",
       "      <td>4</td>\n",
       "      <td>[4, 6, 2, 2]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>{'4': 1, '6': 1, '2': 2, '46': 1, '62': 1, '22...</td>\n",
       "      <td>1.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>shrk</td>\n",
       "      <td>4</td>\n",
       "      <td>[s, h, r, k]</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>{'s': 1, 'h': 1, 'r': 1, 'k': 1, 'sh': 1, 'hr'...</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99995</th>\n",
       "      <td>sifelizestasdecirmeloquerras</td>\n",
       "      <td>28</td>\n",
       "      <td>[s, i, f, e, l, i, z, e, s, t, a, s, d, e, c, ...</td>\n",
       "      <td>0</td>\n",
       "      <td>28</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>{'s': 4, 'i': 3, 'f': 1, 'e': 5, 'l': 2, 'z': ...</td>\n",
       "      <td>3.624519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99996</th>\n",
       "      <td>iwillalwayslovemyboyfriend</td>\n",
       "      <td>26</td>\n",
       "      <td>[i, w, i, l, l, a, l, w, a, y, s, l, o, v, e, ...</td>\n",
       "      <td>0</td>\n",
       "      <td>26</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>{'i': 3, 'w': 2, 'l': 4, 'a': 2, 'y': 3, 's': ...</td>\n",
       "      <td>3.719295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99997</th>\n",
       "      <td>letsyouupdateyourfunNotesandmore</td>\n",
       "      <td>32</td>\n",
       "      <td>[l, e, t, s, y, o, u, u, p, d, a, t, e, y, o, ...</td>\n",
       "      <td>1</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>{'l': 1, 'e': 4, 't': 3, 's': 2, 'y': 2, 'o': ...</td>\n",
       "      <td>3.726410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99998</th>\n",
       "      <td>chocolatesoeusi912134741</td>\n",
       "      <td>24</td>\n",
       "      <td>[c, h, o, c, o, l, a, t, e, s, o, e, u, s, i, ...</td>\n",
       "      <td>0</td>\n",
       "      <td>15</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>{'c': 2, 'h': 1, 'o': 3, 'l': 1, 'a': 1, 't': ...</td>\n",
       "      <td>3.855389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99999</th>\n",
       "      <td>likemaniggabiggzsayhustleordie</td>\n",
       "      <td>30</td>\n",
       "      <td>[l, i, k, e, m, a, n, i, g, g, a, b, i, g, g, ...</td>\n",
       "      <td>0</td>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>{'l': 2, 'i': 4, 'k': 1, 'e': 3, 'm': 1, 'a': ...</td>\n",
       "      <td>3.923231</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100000 rows Ã— 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                               password  length  \\\n",
       "0                                 bybee       5   \n",
       "1                                  n3m0       4   \n",
       "2                                  2509       4   \n",
       "3                                  4622       4   \n",
       "4                                  shrk       4   \n",
       "...                                 ...     ...   \n",
       "99995      sifelizestasdecirmeloquerras      28   \n",
       "99996        iwillalwayslovemyboyfriend      26   \n",
       "99997  letsyouupdateyourfunNotesandmore      32   \n",
       "99998          chocolatesoeusi912134741      24   \n",
       "99999    likemaniggabiggzsayhustleordie      30   \n",
       "\n",
       "                                        character_tokens  uppercase_count  \\\n",
       "0                                        [b, y, b, e, e]                0   \n",
       "1                                           [n, 3, m, 0]                0   \n",
       "2                                           [2, 5, 0, 9]                0   \n",
       "3                                           [4, 6, 2, 2]                0   \n",
       "4                                           [s, h, r, k]                0   \n",
       "...                                                  ...              ...   \n",
       "99995  [s, i, f, e, l, i, z, e, s, t, a, s, d, e, c, ...                0   \n",
       "99996  [i, w, i, l, l, a, l, w, a, y, s, l, o, v, e, ...                0   \n",
       "99997  [l, e, t, s, y, o, u, u, p, d, a, t, e, y, o, ...                1   \n",
       "99998  [c, h, o, c, o, l, a, t, e, s, o, e, u, s, i, ...                0   \n",
       "99999  [l, i, k, e, m, a, n, i, g, g, a, b, i, g, g, ...                0   \n",
       "\n",
       "       lowercase_count  numbers_count  special_character_count  \\\n",
       "0                    5              0                        0   \n",
       "1                    2              2                        0   \n",
       "2                    0              4                        0   \n",
       "3                    0              4                        0   \n",
       "4                    4              0                        0   \n",
       "...                ...            ...                      ...   \n",
       "99995               28              0                        0   \n",
       "99996               26              0                        0   \n",
       "99997               31              0                        0   \n",
       "99998               15              9                        0   \n",
       "99999               30              0                        0   \n",
       "\n",
       "                                       ngram_occurrences   entropy  \n",
       "0      {'b': 2, 'y': 1, 'e': 2, 'by': 1, 'yb': 1, 'be...  1.521928  \n",
       "1      {'n': 1, '3': 1, 'm': 1, '0': 1, 'n3': 1, '3m'...  2.000000  \n",
       "2      {'2': 1, '5': 1, '0': 1, '9': 1, '25': 1, '50'...  2.000000  \n",
       "3      {'4': 1, '6': 1, '2': 2, '46': 1, '62': 1, '22...  1.500000  \n",
       "4      {'s': 1, 'h': 1, 'r': 1, 'k': 1, 'sh': 1, 'hr'...  2.000000  \n",
       "...                                                  ...       ...  \n",
       "99995  {'s': 4, 'i': 3, 'f': 1, 'e': 5, 'l': 2, 'z': ...  3.624519  \n",
       "99996  {'i': 3, 'w': 2, 'l': 4, 'a': 2, 'y': 3, 's': ...  3.719295  \n",
       "99997  {'l': 1, 'e': 4, 't': 3, 's': 2, 'y': 2, 'o': ...  3.726410  \n",
       "99998  {'c': 2, 'h': 1, 'o': 3, 'l': 1, 'a': 1, 't': ...  3.855389  \n",
       "99999  {'l': 2, 'i': 4, 'k': 1, 'e': 3, 'm': 1, 'a': ...  3.923231  \n",
       "\n",
       "[100000 rows x 9 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create input dataframe containing passwords and relevant, extracted features\n",
    "password_inputs = pd.DataFrame()\n",
    "\n",
    "# add the passwords from the dataset\n",
    "password_inputs['password'] = df_passwords['password']\n",
    "\n",
    "# store lengths of each password\n",
    "password_inputs['length'] = password_inputs['password'].apply(len)\n",
    "\n",
    "# tokenize input passwords by characters\n",
    "password_inputs['character_tokens'] = password_inputs['password'].apply(list)\n",
    "\n",
    "# count upper case letters in password\n",
    "password_inputs['uppercase_count'] = password_inputs['password'].apply(count_uppercase)\n",
    "\n",
    "# count lower case letters in password\n",
    "password_inputs['lowercase_count'] = password_inputs['password'].apply(count_lowercase)\n",
    "\n",
    "# count numbers in password\n",
    "password_inputs['numbers_count'] = password_inputs['password'].apply(count_numbers)\n",
    "\n",
    "# count special character in password\n",
    "password_inputs['special_character_count'] = password_inputs['password'].apply(count_special_characters)\n",
    "\n",
    "# find occurrences of each character in the passwords\n",
    "password_inputs['ngram_occurrences'] = password_inputs['character_tokens'].apply(generate_ngrams_counts)\n",
    "\n",
    "# find entropy of each passwords (Shannon Entropy)\n",
    "password_inputs['entropy'] = password_inputs['character_tokens'].apply(shannon_entropy)\n",
    "\n",
    "password_inputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the input matrix and class vector\n",
    "X = password_inputs[['length', 'uppercase_count', 'lowercase_count', 'numbers_count', 'special_character_count', 'entropy']].to_numpy()\n",
    "y = labels.to_numpy().reshape(-1, 1)\n",
    "\n",
    "# split data into train, validation, and test sets\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate accuracy given the true labels and the predictions\n",
    "def accuracy(y_truth, y_pred):\n",
    "    correct_pred = 0\n",
    "    # iterate through the values and check if the labels are the same, update as required\n",
    "    for y_t, y_p in zip(y_truth, y_pred):\n",
    "        if y_t == y_p :\n",
    "            correct_pred += 1\n",
    "    # find the proportion by dividing the correct predictions by all the predictions\n",
    "    return correct_pred / len(y_truth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# softmax function\n",
    "def softmax(z):\n",
    "   exp_z = np.exp(z - np.max(z, axis = 1, keepdims = True))\n",
    "   return exp_z / np.sum(exp_z, axis = 1, keepdims = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict class using softmax and weights\n",
    "def softmax_prediction(X, w):\n",
    "   # add bias terms\n",
    "   X = np.hstack((np.ones((X.shape[0], 1)), X))\n",
    "   # return the class with the highest probability as the predicted label\n",
    "   return np.argmax(softmax(X.dot(w.T)), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# logistic regression function using softmax instead of sigmoid for multinomial classification (gradient descent)\n",
    "def logistic_regression(X, y, num_classes, iterations, learning_rate):\n",
    "   # add bias terms\n",
    "   X = np.hstack((np.ones((X.shape[0], 1)), X))\n",
    "\n",
    "   # initialize the weights\n",
    "   w = np.ones((num_classes, X.shape[1]))\n",
    "\n",
    "   # gradient descent, adjust weights iteratively using the learning rate\n",
    "   for i in range(iterations):\n",
    "      # find the predicted \n",
    "      class_probabilities = softmax(X.dot(w.T))\n",
    "\n",
    "      # one hot encoding of labels\n",
    "      y_one_hot = np.eye(num_classes)[y].reshape(len(y), num_classes)\n",
    "\n",
    "      # calculate gradient and adjust the weights\n",
    "      gradient = (class_probabilities - y_one_hot).T.dot(X) / len(y)\n",
    "      w -= learning_rate * gradient\n",
    "   return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the logistic regression model\n",
    "w = logistic_regression(X_train, y_train, len(labels), 100000, 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Train Accuracy: 0.8196\n",
      "Logistic Regression Validation Accuracy: 0.8261\n",
      "Logistic Regression Test Accuracy: 0.8203\n"
     ]
    }
   ],
   "source": [
    "# find the accuracy metrics for each set of data using logistic regression weights\n",
    "train_predictions = softmax_prediction(X_train, w)\n",
    "train_accuracy = accuracy(y_train.reshape(1, -1)[0], train_predictions)\n",
    "print(\"Logistic Regression Train Accuracy:\", train_accuracy)\n",
    "\n",
    "val_predictions = softmax_prediction(X_val, w)\n",
    "val_accuracy = accuracy(y_val.reshape(1, -1)[0], val_predictions)\n",
    "print(\"Logistic Regression Validation Accuracy:\", val_accuracy)\n",
    "\n",
    "test_predictions = softmax_prediction(X_test, w)\n",
    "test_accuracy = accuracy(y_test.reshape(1, -1)[0], test_predictions)\n",
    "print(\"Logistic Regression Test Accuracy:\", test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# k nearest neighbors function\n",
    "def knn(X, num_classes, k, norm_order):\n",
    "    # initialize list of predictions\n",
    "    y_pred = []\n",
    "    for x in X:\n",
    "        # find the distances to the training data\n",
    "        distances_with_indexes = []\n",
    "        # iterate through the training data and find the distances to each point using the normalization order\n",
    "        for i in range(len(X_train)):\n",
    "            # store the top k data points that are closest to x\n",
    "            if len(distances_with_indexes) < k:\n",
    "                heapq.heappush(distances_with_indexes, (-1 * np.linalg.norm(X_train[i] - x, ord=norm_order), i))\n",
    "            else:\n",
    "                heapq.heappushpop(distances_with_indexes, (-1 * np.linalg.norm(X_train[i] - x, ord=norm_order), i))\n",
    "        # initialize class counts to zero\n",
    "        class_counts = np.zeros(num_classes)\n",
    "        # iterate through the k nearest neighbors and find the counts of each label\n",
    "        for distance, index in distances_with_indexes:\n",
    "            class_counts[y_train[index]] += 1\n",
    "        # append the class with the greatest count in the neighbors as the predicted label for this x\n",
    "        y_pred.append(np.argmax(class_counts))\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN Train Accuracy: 0.9876875\n"
     ]
    }
   ],
   "source": [
    "# find the accuracy metrics for each set of data using knn\n",
    "train_predictions = knn(X_train, 5, 20, 2)\n",
    "train_accuracy = accuracy(y_train.reshape(1, -1)[0], train_predictions)\n",
    "print(\"KNN Train Accuracy:\", train_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN Validation Accuracy: 0.9877\n"
     ]
    }
   ],
   "source": [
    "# find the accuracy metrics for each set of data using knn\n",
    "val_predictions = knn(X_val, 5, 20, 2)\n",
    "val_accuracy = accuracy(y_val.reshape(1, -1)[0], val_predictions)\n",
    "print(\"KNN Validation Accuracy:\", val_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN Test Accuracy: 0.9864\n"
     ]
    }
   ],
   "source": [
    "# find the accuracy metrics for each set of data using knn\n",
    "test_predictions = knn(X_test, 5, 20, 2)\n",
    "test_accuracy = accuracy(y_test.reshape(1, -1)[0], test_predictions)\n",
    "print(\"KNN Test Accuracy:\", test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# relu activation function\n",
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "# derivative of relu for backprop\n",
    "def relu_derivative(x):\n",
    "    return (x > 0).astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLP forward function to pass through inputs and weights/biases to retrieve the outputs of last hidden layer\n",
    "def forward(X, weights1, bias1, weights2, bias2):\n",
    "    # calculate the inputs to the first hidden layer\n",
    "    layer1_logits = X.dot(weights1) + bias1\n",
    "    # find the outputs of the first hidden layer using the activation function\n",
    "    layer1_outputs = relu(layer1_logits)\n",
    "\n",
    "    # calculate the inputs to the second hidden layer\n",
    "    layer2_logits = layer1_outputs.dot(weights2) + bias2\n",
    "    # find the outputs of the second hidden layer using the activation function\n",
    "    layer2_outputs = softmax(layer2_logits)\n",
    "\n",
    "    # return outputs of both layers to calculate the error using backprop\n",
    "    return layer1_outputs, layer2_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLP backward function to calculate the error using backpropagation\n",
    "def backward(X, y, layer1_outputs, layer2_outputs, weights1, bias1, weights2, bias2, learning_rate):\n",
    "    # find the number of samples\n",
    "    samples = y.shape[0]\n",
    "    # calculate the difference between predictions and true labels (one-hot encoded y data)\n",
    "    error = layer2_outputs - y\n",
    "    \n",
    "    # calculate the gradient of weights2 by using the derivative of loss function \n",
    "    weights2_deriv = layer1_outputs.T.dot(error) / samples\n",
    "    # calculate the gradient of bias2\n",
    "    bias2_deriv = np.sum(error, axis=0, keepdims=True) / samples\n",
    "    # calculate the gradient of weights1\n",
    "    weights1_deriv = X.T.dot(error.dot(weights2.T) * relu_derivative(layer1_outputs)) / samples\n",
    "    # calculate the gradient of bias1\n",
    "    bias1_deriv = np.sum(error.dot(weights2.T) * relu_derivative(layer1_outputs), axis=0, keepdims=True) / samples\n",
    "    \n",
    "    # adjust the weights and bias terms using the calculated gradients \n",
    "    weights2 -= learning_rate * weights2_deriv\n",
    "    bias2 -= learning_rate * bias2_deriv\n",
    "    weights1 -= learning_rate * weights1_deriv\n",
    "    bias1 -= learning_rate * bias1_deriv\n",
    "    return weights1, bias1, weights2, bias2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training function for MLP\n",
    "def MLP_train(X, y, num_hidden_nodes, num_classes, learning_rate=0.01, epochs=1000):\n",
    "    # do one-hot encoding of the y labels\n",
    "    y_one_hot = np.eye(num_classes)[y].reshape(len(y), num_classes)\n",
    "    # input size is the number of features for each password\n",
    "    num_features = X.shape[1]\n",
    "\n",
    "    # initialize weight vectors and bias terms\n",
    "    weights1 = np.ones((num_features, num_hidden_nodes))\n",
    "    bias1 = np.ones((1, num_hidden_nodes))\n",
    "    weights2 = np.ones((num_hidden_nodes, num_classes))\n",
    "    bias2 = np.ones((1, num_classes))\n",
    "\n",
    "    # iterate through the epochs and adjust the weights/biases to learn\n",
    "    for epoch in range(epochs):\n",
    "        # do the forward pass to find the predictions\n",
    "        layer1_outputs, layer2_outputs = forward(X, weights1, bias1, weights2, bias2)\n",
    "        # do the backward pass to find the error and adjust the weight vectors/bias terms\n",
    "        weights1, bias1, weights2, bias2 = backward(X, y_one_hot, layer1_outputs, layer2_outputs, weights1, bias1, weights2, bias2, learning_rate)\n",
    "    \n",
    "    # return the final weights and biases after training\n",
    "    return weights1, bias1, weights2, bias2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prediction function that takes the argmax of the outputs of the last layer in the MLP\n",
    "def MLP_predict(X, weights1, bias1, weights2, bias2):\n",
    "    # do the foward pass to find the prediction probability distribution\n",
    "    layer1_outputs, layer2_outputs = forward(X, weights1, bias1, weights2, bias2)\n",
    "    # returns the argmax of the outputs which is the class with the highest probability\n",
    "    return np.argmax(layer2_outputs, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights1, bias1, weights2, bias2 = MLP_train(X_train, y_train, 5, 5, 0.03, 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP Train Accuracy: 0.8471625\n",
      "MLP Validation Accuracy: 0.8495\n",
      "MLP Test Accuracy: 0.8392\n"
     ]
    }
   ],
   "source": [
    "# find the accuracy metrics for each set of data using MLP\n",
    "train_predictions = MLP_predict(X_train, weights1, bias1, weights2, bias2)\n",
    "train_accuracy = accuracy(y_train.reshape(1, -1)[0], train_predictions)\n",
    "print(\"MLP Train Accuracy:\", train_accuracy)\n",
    "\n",
    "val_predictions = MLP_predict(X_val, weights1, bias1, weights2, bias2)\n",
    "val_accuracy = accuracy(y_val.reshape(1, -1)[0], val_predictions)\n",
    "print(\"MLP Validation Accuracy:\", val_accuracy)\n",
    "\n",
    "test_predictions = MLP_predict(X_test, weights1, bias1, weights2, bias2)\n",
    "test_accuracy = accuracy(y_test.reshape(1, -1)[0], test_predictions)\n",
    "print(\"MLP Test Accuracy:\", test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate Gini impurity, calculate error based on random classification \n",
    "def gini_impurity(y):\n",
    "    # find the counts of each class/label\n",
    "    class_counts = np.bincount(y.flatten())\n",
    "    # find the distribution of samples across the classes\n",
    "    probabilities = class_counts / len(y)\n",
    "    # calculate the error and return\n",
    "    return 1 - np.sum(probabilities ** 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# finds the best split in the tree\n",
    "def best_split(X, y, n_features):\n",
    "    # find the number of features for each password\n",
    "    num_features = X.shape[1]\n",
    "    # initialize the minimum impurity (begin with infinity to find smallest impurity)\n",
    "    min_impurity = float('inf')\n",
    "    # initialize the specific feature associated with the minimum impurity\n",
    "    min_impurity_feature = None\n",
    "    # initialize the minimum impurity feature's corresponding threshold value\n",
    "    min_impurity_threshold = None\n",
    "\n",
    "    # choose a random subset of features from all the features\n",
    "    feature_subset = np.random.choice(num_features, n_features, replace=False)\n",
    "\n",
    "    # iterate through the features in the subset\n",
    "    for feature in feature_subset:\n",
    "        # get the column corresponding to the current feature\n",
    "        feature_column = X[:, feature]\n",
    "\n",
    "        # find all unique values in the feature column\n",
    "        thresholds = np.unique(feature_column)\n",
    "\n",
    "        # for each threshold value, it is trying to minimize the gini impurity\n",
    "        for threshold in thresholds:\n",
    "            # find all the samples whose values in the feature column are less than the threshold\n",
    "            left_node_samples = y[feature_column < threshold]\n",
    "            # find all the samples whose values in the feature column are greater than or equal to the threshold\n",
    "            right_node_samples = y[feature_column >= threshold]\n",
    "\n",
    "            # if there are samples on either side of the threshold value, then calculate the impurity and update as required\n",
    "            # essentially filters out invalid thresholds that may be greater/less than all the samples\n",
    "            if len(left_node_samples) != 0 and len(right_node_samples) != 0:\n",
    "                # calculate the impurity of the left node samples\n",
    "                left_impurity = gini_impurity(left_node_samples)\n",
    "                # calculate the impurity of the right node samples\n",
    "                right_impurity = gini_impurity(left_node_samples)\n",
    "                # find the weighted impurity value (expectation)\n",
    "                weighted_impurity = (len(left_node_samples) * left_impurity + len(right_node_samples) * right_impurity) / len(y)\n",
    "\n",
    "                # if the weighted impurity is less than the current minimum impurity, then update the values\n",
    "                if weighted_impurity < min_impurity:\n",
    "                    min_impurity = weighted_impurity\n",
    "                    min_impurity_feature = feature\n",
    "                    min_impurity_threshold = threshold\n",
    "\n",
    "    return min_impurity_feature, min_impurity_threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# builds the decision tree\n",
    "def build_tree(X, y, max_depth, min_samples_split, depth=0, n_features=None):\n",
    "    # if there are no unique y values or there are fewer samples than required to split the tree \n",
    "    # or the depth is greater than the specified max depth, return the majority class of the samples\n",
    "    if len(np.unique(y)) == 1 or len(y) < min_samples_split or depth >= max_depth:\n",
    "        return np.argmax(np.bincount(y.flatten()))\n",
    "\n",
    "    # find the feature and threshold that corresponds to the best split\n",
    "    feature, threshold = best_split(X, y, n_features)\n",
    "\n",
    "    # if no feature is returned, return the majority class of the samples\n",
    "    if feature is None:\n",
    "        return np.argmax(np.bincount(y.flatten()))\n",
    "\n",
    "    # get the column corresponding to the feature\n",
    "    feature_column = X[:, feature]\n",
    "\n",
    "    # find all the samples whose values in the feature column are less than the threshold\n",
    "    left_node_samples_X = X[feature_column < threshold]\n",
    "    left_node_samples_y = y[feature_column < threshold]\n",
    "    \n",
    "    # find all the samples whose values in the feature column are greater than or equal to the threshold\n",
    "    right_node_samples_X = X[feature_column >= threshold]\n",
    "    right_node_samples_y = y[feature_column >= threshold]\n",
    "\n",
    "    # recursively build the left and right sides of the tree\n",
    "    left_tree = build_tree(left_node_samples_X, left_node_samples_y, max_depth, min_samples_split, depth + 1, n_features)\n",
    "    right_tree = build_tree(right_node_samples_X, right_node_samples_y, max_depth, min_samples_split, depth + 1, n_features)\n",
    "\n",
    "    return (feature, threshold, left_tree, right_tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# classify sample with the decision tree\n",
    "def predict_tree(x, tree):\n",
    "    # if the given tree is not a tuple, that means the majority class was returned so return the prediction\n",
    "    if not isinstance(tree, tuple):\n",
    "        return tree\n",
    "    \n",
    "    # unpack the tuple\n",
    "    feature, threshold, left_tree, right_tree = tree\n",
    "\n",
    "    # determine which side of the tree to iterate based on relation of sample's feature value to threshold\n",
    "    if x[feature] < threshold:\n",
    "        return predict_tree(x, left_tree)\n",
    "    else:\n",
    "        return predict_tree(x, right_tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trains random forest\n",
    "def random_forest(X, y, n_trees, max_depth, min_samples_split, max_features):\n",
    "    # initialize list of trees\n",
    "    trees = []\n",
    "    # create n number of decision trees\n",
    "    for tree in range(n_trees):\n",
    "        # sample with replacement to increase diversity and reduce overfitting\n",
    "        sample_indices = np.random.choice(len(X), len(X), replace=True)\n",
    "        # build a decision tree with the randomly chosen samples\n",
    "        tree = build_tree(X[sample_indices], y[sample_indices], max_depth, min_samples_split, n_features=max_features)\n",
    "        # store the decision tree\n",
    "        trees.append(tree)\n",
    "    return trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# classify samples with the random forest\n",
    "def predict_forest(X, trees):\n",
    "    # predicts class for each sample using each tree\n",
    "    predictions = np.array([predict_tree(x, tree) for tree in trees for x in X])\n",
    "    predictions = predictions.reshape(len(X), len(trees))\n",
    "    # returns the majority class for each sample as a list\n",
    "    return [np.argmax(np.bincount(row.flatten())) for row in predictions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "trees = random_forest(X_train, y_train, 30, 10, 10, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Train Accuracy: 0.19945\n",
      "Random Forest Validation Accuracy: 0.1988\n",
      "Random Forest Test Accuracy: 0.2052\n"
     ]
    }
   ],
   "source": [
    "# find the accuracy metrics for each set of data using Random Forest\n",
    "train_predictions = predict_forest(X_train, trees)\n",
    "train_accuracy = accuracy(y_train.reshape(1, -1)[0], train_predictions)\n",
    "print(\"Random Forest Train Accuracy:\", train_accuracy)\n",
    "\n",
    "val_predictions = predict_forest(X_val, trees)\n",
    "val_accuracy = accuracy(y_val.reshape(1, -1)[0], val_predictions)\n",
    "print(\"Random Forest Validation Accuracy:\", val_accuracy)\n",
    "\n",
    "test_predictions = predict_forest(X_test, trees)\n",
    "test_accuracy = accuracy(y_test.reshape(1, -1)[0], test_predictions)\n",
    "print(\"Random Forest Test Accuracy:\", test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
