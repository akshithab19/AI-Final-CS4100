{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load saved training/validation/test data\n",
    "with open('train_val_test_data.pkl', 'rb') as file:\n",
    "    X_train, X_val, X_test, y_train, y_val, y_test = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loads the data from the pkl file into X_train, X_val, and X_test\n",
    "full_columns = ['password', 'length', 'length_bin', 'uppercase_count', 'lowercase_count', 'numbers_count', 'special_character_count', 'entropy', \n",
    "                'ngram_occurrences', 'character_diversity', 'contains_name', \n",
    "                'upper_special', 'length_entropy', 'lower_numbers', 'entropy_special',\n",
    "                'upper_ratio', 'lower_ratio', 'special_character_ratio', 'numbers_ratio', 'entropy_per_character']\n",
    "X_train_full_df = pd.DataFrame(X_train, columns=full_columns)\n",
    "X_val_full_df = pd.DataFrame(X_val, columns=full_columns)\n",
    "X_test_full_df = pd.DataFrame(X_test, columns=full_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model 1: Character-Based Features \n",
    "# create X_train, X_val, and X_test for model 1\n",
    "model1_columns = ['length', 'entropy', 'contains_name']\n",
    "X_train_model1 = X_train_full_df[model1_columns].to_numpy().astype(np.float64)\n",
    "X_val_model1 = X_val_full_df[model1_columns].to_numpy().astype(np.float64)\n",
    "X_test_model1 = X_test_full_df[model1_columns].to_numpy().astype(np.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model 2: Semantic Features\n",
    "model2_columns = ['uppercase_count', 'lowercase_count', 'numbers_count', 'special_character_count', 'contains_name']\n",
    "X_train_model2 = X_train_full_df[model2_columns].to_numpy().astype(np.float64)\n",
    "X_val_model2 = X_val_full_df[model2_columns].to_numpy().astype(np.float64)\n",
    "X_test_model2 = X_test_full_df[model2_columns].to_numpy().astype(np.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model 3: Combined Features\n",
    "\n",
    "model3_columns = ['upper_special', 'length_entropy', 'entropy_special', 'upper_ratio', 'lower_ratio', \n",
    "                  'special_character_ratio', 'numbers_ratio', 'entropy_per_character']\n",
    "X_train_model3 = X_train_full_df[model3_columns].to_numpy().astype(np.float64)\n",
    "X_val_model3 = X_val_full_df[model3_columns].to_numpy().astype(np.float64)\n",
    "X_test_model3 = X_test_full_df[model3_columns].to_numpy().astype(np.float64)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model 4: Model with all of the features available\n",
    "full_model_columns = ['length', 'uppercase_count', 'uppercase_count', 'lowercase_count', 'numbers_count', 'special_character_count', 'contains_name',\n",
    "                      'upper_special', 'length_entropy', 'lower_numbers', 'entropy_special', 'upper_ratio', 'lower_ratio', \n",
    "                      'special_character_ratio', 'numbers_ratio', 'entropy_per_character']\n",
    "X_train_full_model = X_train_full_df[full_model_columns].to_numpy().astype(np.float64)\n",
    "X_val_full_model = X_val_full_df[full_model_columns].to_numpy().astype(np.float64)\n",
    "X_test_full_model = X_test_full_df[full_model_columns].to_numpy().astype(np.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate accuracy given the true labels and the predictions\n",
    "def accuracy(y_truth, y_pred):\n",
    "    correct_pred = 0\n",
    "    # iterate through the values and check if the labels are the same, update as required\n",
    "    for y_t, y_p in zip(y_truth, y_pred):\n",
    "        if y_t == y_p :\n",
    "            correct_pred += 1\n",
    "    # find the proportion by dividing the correct predictions by all the predictions\n",
    "    return correct_pred / len(y_truth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# softmax function\n",
    "def softmax(z):\n",
    "   exp_z = np.exp(z - np.max(z, axis = 1, keepdims = True))\n",
    "   return exp_z / np.sum(exp_z, axis = 1, keepdims = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# logistic regression function using softmax instead of sigmoid for multinomial classification (gradient descent)\n",
    "def logistic_regression(X, y, num_classes, iterations, learning_rate):\n",
    "   # add bias terms\n",
    "   X = np.hstack((np.ones((X.shape[0], 1)), X))\n",
    "\n",
    "   # initialize the weights\n",
    "   w = np.ones((num_classes, X.shape[1]))\n",
    "\n",
    "   # gradient descent, adjust weights iteratively using the learning rate\n",
    "   for i in range(iterations):\n",
    "      # find the predicted \n",
    "      class_probabilities = softmax(X.dot(w.T))\n",
    "\n",
    "      # one hot encoding of labels\n",
    "      y_one_hot = np.eye(num_classes)[y].reshape(len(y), num_classes)\n",
    "\n",
    "      # calculate gradient and adjust the weights\n",
    "      gradient = (class_probabilities - y_one_hot).T.dot(X) / len(y)\n",
    "      w -= learning_rate * gradient\n",
    "   return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# single model predictor -- predict class using softmax and weights\n",
    "def softmax_prediction(X, w):\n",
    "   # add bias terms\n",
    "   X = np.hstack((np.ones((X.shape[0], 1)), X))\n",
    "   # return the class with the highest probability as the predicted label\n",
    "   return np.argmax(softmax(X.dot(w.T)), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# makes predictions based on weighted sum\n",
    "def lr_weighted_sum_voting_predictions(X_models, w_models, weights=[0.33, 0.34, 0.33]):\n",
    "    # finds the total number of models based on size of X_models input\n",
    "    num_models = len(X_models)\n",
    "\n",
    "    # creates array of predictions for each of the models\n",
    "    predictions = []\n",
    "    for i in range(num_models):\n",
    "        curr_model_prediction = softmax_prediction(X_models[i], w_models[i])\n",
    "        predictions.append(curr_model_prediction)\n",
    "\n",
    "    # finds expected value for predictions based on given weights\n",
    "    return (weights[0] * predictions[0] + weights[1] * predictions[1] + weights[2] * predictions[2]).astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creates predictions with votes, equal\n",
    "def lr_hard_voting_predictions(X_models, w_models):\n",
    "\n",
    "    # create an array of predictions for all three models\n",
    "    num_models = len(X_models)\n",
    "    predictions = []\n",
    "\n",
    "    for i in range(num_models):\n",
    "        curr_model_prediction = softmax_prediction(X_models[i], w_models[i])\n",
    "        predictions.append(curr_model_prediction)\n",
    "\n",
    "    # reshapes the predictions for voting\n",
    "    predictions = np.stack(predictions, axis=0) \n",
    "\n",
    "    _, num_samples = predictions.shape\n",
    "    num_classes = 5  \n",
    "\n",
    "    final_preds = []\n",
    "    for sample_ind in range(num_samples):\n",
    "\n",
    "        # creates an array to tally up the counts for each of the classes\n",
    "        vote_counts = np.zeros(num_classes)\n",
    "\n",
    "        # iterates through the models\n",
    "        for model_ind in range(num_models):\n",
    "            # finds the predicted class and adds 1 to the class count\n",
    "            class_pred = predictions[model_ind, sample_ind]\n",
    "            vote_counts[class_pred] += 1\n",
    "        # appends to final prediction\n",
    "        final_preds.append(np.argmax(vote_counts))\n",
    "\n",
    "    return np.array(final_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creates predictions with votes, biased towards more accurate models\n",
    "def lr_weighted_hard_voting(X_models, w_models, weights):\n",
    "    # creates predictions\n",
    "    num_models = len(X_models)\n",
    "    predictions = []\n",
    "    for i in range(num_models):\n",
    "        curr_model_prediction = softmax_prediction(X_models[i], w_models[i])\n",
    "        predictions.append(curr_model_prediction)\n",
    "\n",
    "    # reshapes the predictions for voting\n",
    "    predictions = np.stack(predictions, axis=0) \n",
    "\n",
    "    _, num_samples = predictions.shape\n",
    "    num_classes = 5  \n",
    "\n",
    "    final_preds = []\n",
    "    for sample_ind in range(num_samples):\n",
    "\n",
    "        # creates an array to tally up the counts for each of the classes\n",
    "        vote_counts = np.zeros(num_classes)\n",
    "\n",
    "        # iterates through the models\n",
    "        for model_ind in range(num_models):\n",
    "            # finds the predicted class and adds weight to the class count\n",
    "            class_pred = predictions[model_ind, sample_ind]\n",
    "            vote_counts[class_pred] += weights[i]\n",
    "        # appends to final prediction\n",
    "        final_preds.append(np.argmax(vote_counts))\n",
    "\n",
    "    return np.array(final_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train log reg model 1\n",
    "weights_model1 = logistic_regression(X_train_model1, y_train, 5, 100000, 0.025)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train log reg model 2\n",
    "weights_model2 = logistic_regression(X_train_model2, y_train, 5, 100000, 0.045)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train log reg model 3\n",
    "weights_model3 = logistic_regression(X_train_model3, y_train, 5, 100000, 0.005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train log reg model full\n",
    "weights_full_model = logistic_regression(X_train_full_model, y_train, 5, 100000, 0.03)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 0.8313\n",
      "Validation: 0.8307\n",
      "Test: 0.8293\n"
     ]
    }
   ],
   "source": [
    "# predicts based on weighted sum voting \n",
    "train_predictions = lr_weighted_sum_voting_predictions([X_train_model1, X_train_model2, X_train_model3], \n",
    "                                           [weights_model1, weights_model2, weights_model3],\n",
    "                                           [0.2, 0.6, 0.2])\n",
    "train_accuracy = accuracy(y_train.reshape(1, -1)[0], train_predictions)\n",
    "print(\"Train:\", train_accuracy)\n",
    "val_predictions = lr_weighted_sum_voting_predictions([X_val_model1, X_val_model2, X_val_model3], \n",
    "                                           [weights_model1, weights_model2, weights_model3],\n",
    "                                           [0.2, 0.6, 0.2])\n",
    "val_accuracy = accuracy(y_val.reshape(1, -1)[0], val_predictions)\n",
    "print(\"Validation:\", val_accuracy)\n",
    "test_predictions = lr_weighted_sum_voting_predictions([X_test_model1, X_test_model2, X_test_model3], \n",
    "                                           [weights_model1, weights_model2, weights_model3],\n",
    "                                           [0.2, 0.6, 0.2])\n",
    "test_accuracy = accuracy(y_test.reshape(1, -1)[0], test_predictions)\n",
    "print(\"Test:\", test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 0.9135625\n",
      "Validation: 0.9149\n",
      "Test: 0.8293\n"
     ]
    }
   ],
   "source": [
    "# predicts based on hard voting, equal \n",
    "train_predictions = lr_hard_voting_predictions([X_train_model1, X_train_model2, X_train_model3], \n",
    "                                           [weights_model1, weights_model2, weights_model3])\n",
    "train_accuracy = accuracy(y_train.reshape(1, -1)[0], train_predictions)\n",
    "print(\"Train:\", train_accuracy)\n",
    "val_predictions = lr_hard_voting_predictions([X_val_model1, X_val_model2, X_val_model3], \n",
    "                                           [weights_model1, weights_model2, weights_model3])\n",
    "val_accuracy = accuracy(y_val.reshape(1, -1)[0], val_predictions)\n",
    "print(\"Validation:\", val_accuracy)\n",
    "train_predictions = lr_hard_voting_predictions([X_train_model1, X_train_model2, X_train_model3], \n",
    "                                           [weights_model1, weights_model2, weights_model3])\n",
    "train_accuracy = accuracy(y_train.reshape(1, -1)[0], train_predictions)\n",
    "print(\"Test:\", test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 0.9135625\n",
      "Validation: 0.9149\n",
      "Test: 0.9138\n"
     ]
    }
   ],
   "source": [
    "# predicts based on hard voting, weighted/biased\n",
    "\n",
    "weights = [1.5, 2, 1.5]\n",
    "train_predictions = lr_weighted_hard_voting([X_train_model1, X_train_model2, X_train_model3], \n",
    "                                           [weights_model1, weights_model2, weights_model3],\n",
    "                                           weights)\n",
    "train_accuracy = accuracy(y_train.reshape(1, -1)[0], train_predictions)\n",
    "print(\"Train:\", train_accuracy)\n",
    "\n",
    "val_predictions = lr_weighted_hard_voting([X_val_model1, X_val_model2, X_val_model3], \n",
    "                                           [weights_model1, weights_model2, weights_model3],\n",
    "                                           weights)\n",
    "val_accuracy = accuracy(y_val.reshape(1, -1)[0], val_predictions)\n",
    "print(\"Validation:\", val_accuracy)\n",
    "test_predictions = lr_weighted_hard_voting([X_test_model1, X_test_model2, X_test_model3], \n",
    "                                           [weights_model1, weights_model2, weights_model3],\n",
    "                                           weights)\n",
    "test_accuracy = accuracy(y_test.reshape(1, -1)[0], test_predictions)\n",
    "print(\"Test:\", test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 0.9174875\n",
      "Validation: 0.9213\n",
      "Test: 0.9156\n"
     ]
    }
   ],
   "source": [
    "# model 1 - find the accuracy metrics for each set of data using logistic regression weights for a single model\n",
    "train_predictions = softmax_prediction(X_train_model1, weights_model1)\n",
    "train_accuracy = accuracy(y_train.reshape(1, -1)[0], train_predictions)\n",
    "print(\"Train:\", train_accuracy)\n",
    "\n",
    "val_predictions = softmax_prediction(X_val_model1, weights_model1)\n",
    "val_accuracy = accuracy(y_val.reshape(1, -1)[0], val_predictions)\n",
    "print(\"Validation:\", val_accuracy)\n",
    "\n",
    "test_predictions = softmax_prediction(X_test_model1, weights_model1)\n",
    "test_accuracy = accuracy(y_test.reshape(1, -1)[0], test_predictions)\n",
    "print(\"Test:\", test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 0.878225\n",
      "Validation: 0.8763\n",
      "Test: 0.8775\n"
     ]
    }
   ],
   "source": [
    "# model 2 - find the accuracy metrics for each set of data using logistic regression weights for a single model\n",
    "train_predictions = softmax_prediction(X_train_model2, weights_model2)\n",
    "train_accuracy = accuracy(y_train.reshape(1, -1)[0], train_predictions)\n",
    "print(\"Train:\", train_accuracy)\n",
    "\n",
    "val_predictions = softmax_prediction(X_val_model2, weights_model2)\n",
    "val_accuracy = accuracy(y_val.reshape(1, -1)[0], val_predictions)\n",
    "print(\"Validation:\", val_accuracy)\n",
    "\n",
    "test_predictions = softmax_prediction(X_test_model2, weights_model2)\n",
    "test_accuracy = accuracy(y_test.reshape(1, -1)[0], test_predictions)\n",
    "print(\"Test:\", test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 0.7783625\n",
      "Validation: 0.7772\n",
      "Test: 0.7755\n"
     ]
    }
   ],
   "source": [
    "# model 3 - find the accuracy metrics for each set of data using logistic regression weights for a single model\n",
    "train_predictions = softmax_prediction(X_train_model3, weights_model3)\n",
    "train_accuracy = accuracy(y_train.reshape(1, -1)[0], train_predictions)\n",
    "print(\"Train:\", train_accuracy)\n",
    "\n",
    "val_predictions = softmax_prediction(X_val_model3, weights_model3)\n",
    "val_accuracy = accuracy(y_val.reshape(1, -1)[0], val_predictions)\n",
    "print(\"Validation:\", val_accuracy)\n",
    "\n",
    "test_predictions = softmax_prediction(X_test_model3, weights_model3)\n",
    "test_accuracy = accuracy(y_test.reshape(1, -1)[0], test_predictions)\n",
    "print(\"Test:\", test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 0.6872875\n",
      "Validation: 0.6809\n",
      "Test : 0.6864\n"
     ]
    }
   ],
   "source": [
    "# model 4 (full) - find the accuracy metrics for each set of data using logistic regression weights for a single model\n",
    "train_predictions = softmax_prediction(X_train_full_model, weights_full_model)\n",
    "train_accuracy = accuracy(y_train.reshape(1, -1)[0], train_predictions)\n",
    "print(\"Train:\", train_accuracy)\n",
    "\n",
    "val_predictions = softmax_prediction(X_val_full_model, weights_full_model)\n",
    "val_accuracy = accuracy(y_val.reshape(1, -1)[0], val_predictions)\n",
    "print(\"Validation:\", val_accuracy)\n",
    "\n",
    "test_predictions = softmax_prediction(X_test_full_model, weights_full_model)\n",
    "test_accuracy = accuracy(y_test.reshape(1, -1)[0], test_predictions)\n",
    "print(\"Test :\", test_accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
